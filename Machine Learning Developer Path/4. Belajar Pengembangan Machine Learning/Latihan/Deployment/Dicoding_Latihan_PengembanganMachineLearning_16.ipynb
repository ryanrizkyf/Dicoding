{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Dicoding-Latihan-PengembanganMachineLearning-16.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kze6PF1F5GRG"
      },
      "source": [
        "# Deployment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGdYK5KU5Ovq"
      },
      "source": [
        "# Data Pipelines with Tensorflow Data Services"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1346pSyF5bfg"
      },
      "source": [
        "# Extract, Transform, and Load (ETL)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "urjDP8PSTgoq"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BR3AWtk6Tmh5"
      },
      "source": [
        "#Proses Extract\n",
        "dataset = tfds.load(name = \"mnist\", split = \"train\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kvVV4rPpTvMw"
      },
      "source": [
        "#Proses Transform\n",
        "dataset = dataset.shuffle(1000)\n",
        "dataset = dataset.repeat(10)\n",
        "dataset = dataset.batch(BATCH_SIZE)\n",
        "\n",
        "# dataset = dataset.map(lambda x: tf.parse_single_example(x, features))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3NIGI8omoPFS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        },
        "outputId": "6972ad34-be3c-455a-d831-e7ae7d6c50fd"
      },
      "source": [
        "#Proses Load\n",
        "iterator = dataset.make_one_shot_iterator()\n",
        "features = iterator.get_next()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-21-c9fc2c7514f9>:2: DatasetV1.make_one_shot_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_one_shot_iterator(dataset)`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-21-c9fc2c7514f9>:2: DatasetV1.make_one_shot_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_one_shot_iterator(dataset)`.\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}